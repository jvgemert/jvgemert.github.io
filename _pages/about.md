---
layout: about
title: about
permalink: /
subtitle: Head of the <a href="http://visionlab.tudelft.nl">Computer vision lab</a> at Delft University of Technology, The Netherlands. 

profile:
  align: left
  image: jan.jpg
  image_circular: false # crops the image to make it circular
  more_info: >
    <p>Office: 28.6.E340</p>
    <p>Van Mourik Broekmanweg 6</p>
    <p>2628 XE Delft, NL</p>

selected_papers: false # includes a list of papers marked as "selected={true}"
social: true # includes social icons at the bottom of the page

announcements:
  enabled: true # includes a list of news items
  scrollable: false # adds a vertical scroll bar if there are more than 3 news items
  limit:  # leave blank to include all the news in the `_news` folder

latest_posts:
  enabled: false
  scrollable: true # adds a vertical scroll bar if there are more than 3 new posts items
  limit: 3 # leave blank to include all the blog posts
---

I am studying visual recognition AI and my research style is characterized by:


**Fundamental empirical understanding-based deep learning research.**
I do fundamental research; I study the underlying visual AI models used in specific applications. I find Deep Learning instrumental in Computer Vision, where the end-to-end paradigm is essential to allow information to flow between learning the extracted features on the one end (input) that contribute to the visual task at the other end (output). I am an empiricist and do understanding-based research, this means that I ask for (controlled) experiments that validate each step along the way; and search for experimental evidence to try to understand and motivate why a possible improvement happens, and why and when it does not. See my [research guidelines](/links.html).

**Find &amp; evaluate powerful yet flexible physical priors for data-efficient visual recognition AI.**
The foundation of current visual AI is built on top of huge, uncurated data. This data is predominantly controlled by a handful of private companies, what makes us dependent on them. Because of the size, this data is computationally demanding to process, and difficult to curate for bias, privacy, or otherwise sensitive data. My research is on exploiting prior knowledge from the image formation process to reduce the amount of data needed to train visual AI: All knowledge that is built-in no longer has to be learned from data. With respect to built-in knowledge, my interpretation of [Rich Sutton's "Bitter Lesson"](http://www.incompleteideas.net/IncIdeas/BitterLesson.html) is that we should scale those AI models that extract the strongest learning signal from each and every valuable data sample. 

