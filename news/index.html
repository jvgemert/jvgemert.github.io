<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> news | Jan van Gemert </title> <meta name="author" content="Jan van Gemert"> <meta name="description" content="Academic website of Jan van Gemert, Computer Vision and Deep Learning research, TU Delft. "> <meta name="keywords" content="Jan van Gemert, Computer vision, Deep Learning, visual inductive priors, data-efficient foundation models, meta-science for machine learning, research guidelines"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon-96x96.png?92eebb8041c4ac7fa4691a4264dd3c49"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jvgemert.github.io/news/"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jan</span> van Gemert </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/links.html">guidelines </a> </li> <li class="nav-item "> <a class="nav-link" href="/MSCthesis.html">MSc-thesis </a> </li> <li class="nav-item "> <a class="nav-link" href="/meeting.html">meetings </a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">news</h1> <p class="post-description"></p> </header> <article> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <strong>PhD Committee.</strong> For the PhD defense of Dr. T. Schoonbeek, Technical University Eindhoven (TUE).</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <strong>Metascience for ML.</strong> I made the website for our <a href="https://metascienceforml.github.io/" rel="external nofollow noopener" target="_blank">Metascience for machine learning</a> initiative.</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <strong>PhD Committee.</strong> For the PhD defense of Dr. Junhan Wen, Delft University of Technology (TUD).</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <strong>Keynote.</strong> At the Benchmarking Initiative for Multimedia Evaluation (<a href="https://multimediaeval.github.io/" rel="external nofollow noopener" target="_blank">MediaEval</a>) with the title “<a href="/assets/pdf/keynote-mediaEval25.pdf">AI benchmarking for hypothesis-driven science in machine and deep learning?</a>”.</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <strong>PhD Committee.</strong> For the PhD defense of Dr. L.J.A. Weissbart, Delft University of Technology (TUD).</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Talk.</b> at the performance marketing day at Booking.com about looking under the hood of Deep Learning and AI.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Erkut Akdag, Technical University Eindhoven (TUE). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Talk.</b> at the <a href="https://www.tudelft.nl/evenementen/2025/delft-ai/kick-off-workshop-metascience-for-machine-learning" rel="external nofollow noopener" target="_blank">Ellis workshop on meta science for machine learning</a>, I gave a talk about my view on <a href="/assets/pdf/Meta%20science%20in%20Machine%20and%20Deep%20Learning.pdf">Science and Meta-Science in Machine and Deep Learning</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Elsbeth van Dam, Radboud University Nijmegen. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Guidelines.</b> Tips for grant proposal writing. I gave a presentation for all TU Delft computer science faculty on individual grant proposal writing: <a href="/assets/pdf/Fairytales%20and%20successful%20grant%20proposals.pdf">Fairytales.. ..and successful individual grant proposals</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Organization.</b> I’m co-organizing an <a href="https://www.tudelft.nl/evenementen/2025/delft-ai/kick-off-workshop-metascience-for-machine-learning" rel="external nofollow noopener" target="_blank">Ellis workshop on meta science for machine learning.</a><br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at ICCV 2025.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Prakash Chandra Chhipa, University of Lulea, Sweden. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Accepted paper.</b> Computer Vision Pattern and Recognition workshop (CVPRw), “Making Every Event Count: Balancing Data Efficiency and Accuracy in Event Camera Subsampling”. [<a href="pub/Subsampling_types_CVPRw2025.pdf">pdf</a>, <a href="https://github.com/hesamaraghi/event-camera-subsampling-methods" rel="external nofollow noopener" target="_blank">code</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Radio interview.</b> On <a href="https://www.omroepdelft.nl/" rel="external nofollow noopener" target="_blank">Omroep Delft</a> about my research about reducing data for AI.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Grant.</b> Obtained a prestigious <a href="https://www.tudelft.nl/en/2025/tu-delft/four-vici-grants-for-leading-tu-delft-researchers" rel="external nofollow noopener" target="_blank">NWO VICI</a> grant on <a href="https://www.tudelft.nl/en/2025/eemcs/breaking-ai-barriers-with-foundational-models-and-depth-understanding" rel="external nofollow noopener" target="_blank">‘Project dAIta: Data Efficient AI Foundation Models’</a>, on how to make foundation models less relient on huge datasets.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Accepted paper.</b> Computer Vision Pattern and Recognition (CVPR), “End-to-End Implicit Neural Representations for Classification”. [<a href="pub/meta_learned_SIREN__CVPR25_.pdf">pdf</a>, <a href="https://github.com/SanderGielisse/MWT" rel="external nofollow noopener" target="_blank">code</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Accepted paper.</b> Computer Vision Pattern and Recognition (CVPR), “Learning Physics From Video: Unsupervised Physical Parameter Estimation for Continuous Dynamical Systems”. [<a href="pub/Learning_physics_from_video_and_dataset_CVPR_2025.pdf">pdf</a>, <a href="https://github.com/Alejandro-neuro/Learning_physics_from_video" rel="external nofollow noopener" target="_blank">code</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Accepted paper.</b> ICLR 2025 Workshop on Weight Space Learning, “ARC: Anchored Representation Clouds for High-Resolution INR Classification”. [<a href="pub/ARC_paper_Joost_Luijmes_ICLR.pdf">pdf</a>, <a href="https://github.com/JLuij/anchored_representation_clouds" rel="external nofollow noopener" target="_blank">code</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Guidelines.</b> I added a new machine and deep learning meta-science guideline document on <a href="storyline.pdf">The Storyline: the beating heart of the research process</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Guidelines.</b> To add my <a href="reviewRebut.pdf">rebuttal guidelines</a>, I now include a few of my own rebuttal examples: <a href="/assets/pdf/rebuttalExamples/rebuttalExample1.pdf">Example 1</a>, <a href="/assets/pdf/rebuttalExamples/rebuttalExample2.pdf">Example 2</a>, <a href="/assets/pdf/rebuttalExamples/rebuttalExample3.pdf">Example 3</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Grant.</b> Got 2 PHD students funded through the FIND <a href="https://www.nwo.nl/en/news/investing-in-innovation-19-million-for-technology-developments-with-societal-and-economic-impact" rel="external nofollow noopener" target="_blank">NWO Perspectief grant</a> doing reseach on AI foundation models.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Qi Bi, at University of Amsterdam. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2025</th> <td> <ul> <li> <b>Organization.</b> I’m a ‘lead area chair’ at CVPR 2025.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Keynote.</b> At the International Conference on Pattern Recognition <a href="https://sites.google.com/unifi.it/fbe2024/program" rel="external nofollow noopener" target="_blank">FBE 2024 workshop</a>, titled “Deep House of Cards? Testing the fundaments in image and video AI”, <a href="/assets/pdf/slides-icpr24.pdf">slides</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Mentoring.</b> I am a mentor at the <a href="https://www.tudelft.nl/ewi/over-de-faculteit/afdelingen/software-technology/f-cube" rel="external nofollow noopener" target="_blank">Future Female+ Faculty mentorship program</a> at TU Delft.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Zimin Xia, at TU Delft. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Reviewer.</b> for “IEEE Transactions on Pattern Analysis and Machine Intelligence” journal.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Reviewer.</b> for “Neural Networks” journal.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Tom van Dijk, at TU Delft. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Wonderful that Dr. Ombretta Strafforello graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Public science communication.</b> “AI and Vision: A shallow dive into deep learning” at <a href="https://www.harvestimaging.com/forum_introduction_2024.php" rel="external nofollow noopener" target="_blank">Harvest Imaging Forum</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Mentoring.</b> I am a mentor at the <a href="https://eccv2024.ecva.net/Conferences/2024/SpeedMentoringSchedule" rel="external nofollow noopener" target="_blank">speed mentoring event</a> at the European Conference on Computer Vision (ECCV) 2024.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> European Conference on Computer Vision (ECCV), “MSD: A Benchmark Dataset for Floor Plan Generation of Building Complexes” [<a href="pub/MSD_ECCV24.pdf">pdf</a>, <a href="https://caspervanengelenburg.github.io/msd-eccv24-page/" rel="external nofollow noopener" target="_blank">project</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> European Conference on Computer Vision workshop (ECCVw), “Aligning Object Detector Bounding Boxes with Human Preference” [<a href="pub/Aligning_Object_Detectors_with_Humans.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> European Conference on Computer Vision workshop (ECCVw), “Pushing Joint Image Denoising and Classification to the Edge” [<a href="pub/denoiseNASeccv24.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> European Conference on Computer Vision workshop (ECCVw), “Pushing the boundaries of event subsampling in event-based video classification using CNNs” [<a href="pub/CNN_sparsity_ECCV24.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> European Conference on Computer Vision workshop (ECCVw), “HAVANA: Hierarchical stochastic neighbor embedding for Accelerated Video ANnotAtions” [<a href="pub/havanaECCV2024.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Proud that Dr. Yunqiang Li graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Sander Klomp. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Radio interview.</b> On Radio Totaal Normaal, <a href="https://radiototaalnormaal.nl/uitzending-11-april/" rel="external nofollow noopener" target="_blank">Kunstmatige intelligentie</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Proud that Dr. Xin Liu graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Research guidelines.</b> I have consolidated all my separate research guidelines in a single document: Empirical understanding-based <a href="ResearchGuidelinesInDL.pdf">Research guidelines in Deep Learning</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Proud that Dr. Attila Lengyel graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Proud that Dr. Ziqi Wang graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Grant.</b> Received a <a href="https://www.biodiversa.eu/" rel="external nofollow noopener" target="_blank">EU grant</a> Biodiversa+: European Biodiversity Partnership.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Organization.</b> I’m a senior area chair at ECCV 2024.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Organization.</b> Started a new term as chairman of the <a href="https://asci.tudelft.nl/?page_id=254" rel="external nofollow noopener" target="_blank">ASCI</a> scientific committee.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision Theory and Applications (VISAPP), “End-to-end-chess-recognition” [<a href="pub/end_to_end_chess_recognition.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision Theory and Applications (VISAPP), “Scale Learning in Scale-Equivariant Convolutional networks” [<a href="pub/Scale_Learning_in_Scale_Equivariant_Convolutional_networks.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Raffaele Imbriaco. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2024</th> <td> <ul> <li> <b>Organization.</b> I’m a reviewer at CVPR 2024.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Grant.</b> Received a <a href="https://www.nwo.nl/nieuws/42-nieuwe-take-off-projecten-kunnen-van-start" rel="external nofollow noopener" target="_blank">Take-off grant</a> for developing Video AI applications.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> Neural Information Processing Systems (NeurIPS), “Color Equivariant Convolutional Networks” [<a href="pub/Color_Equivariant_Convolutional_Networks___NeurIPS_2023.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Invited talk.</b> “A shallow introduction to Deep Learning” at <a href="https://asci.tudelft.nl/project/a16-spring-school-on-heterogeneous-computing-systems/" rel="external nofollow noopener" target="_blank">ASCI Winterschool on Deep Learning</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Rick Groenendijk. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision (ICCV), “Objects do not disappear: Video object detection by single-frame object location anticipation” [<a href="pub/ICCV_2023_Video_Object_Detection_.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision (ICCV), “A step towards understanding why classification helps regression” [<a href="pub/ImbalancedRegression_ICCV23.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision (ICCV), “Differentiable Transportation Pruning” [<a href="pub/DifferentiablePruning_ICCV2023.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Video BagNet: short temporal receptive fields increase robustness in long-term action recognition” [<a href="pub/Video_BagNet___VIPrior_2023.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Are current long-term video understanding datasets long-term?” [<a href="pub/Long_term_action_recognition_datasets___CVEU_2023.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Is there progress in activity progress prediction?” [<a href="pub/ICCVw23_Progress_Prediction.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Using and abusing equivariance” [<a href="pub/ICCV_Exact_Equivariance.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Benchmarking Data Efficiency and Computational Efficiency of Temporal Action Localization Models” [<a href="pub/benchmark-data-efficiency-tal.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “SSIG: A Visually-Guided Graph Edit Distance for Floor Plan Similarity” [<a href="pub/iccvw_23_ssig_camera.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Computer Vision workshop (ICCVw), “Non-Destructive Infield Quality Estimation of Strawberries Using Deep Architectures” [<a href="pub/CVPPA_2023_Strawberry.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Invited talk.</b> “Computer vision by deep learning” at <a href="https://www.plantum.nl/" rel="external nofollow noopener" target="_blank">Plantum</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Award.</b>Best educator in the MSc <a href="/assets/img/educator23award.jpeg">award</a> Computer Science MSc 2023. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at BMVC 2023.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> Annals of Clinical and Translational Neurology, “Assessing facial weakness in myasthenia gravis with facial recognition software and deep learning” [<a href="pub/FacialWeaknessMyasthenia23.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> The Netherlands Conference on Computer Vision <a href="https://sites.google.com/view/nccv2023/home" rel="external nofollow noopener" target="_blank">NCCV 2023</a> <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> ICCV workshop of our 4th iteration of the Visual Inductive Priors for Data-Efficient Deep Learning: <a href="https://vipriors.github.io/" rel="external nofollow noopener" target="_blank">VIPriors</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> ICCV workshop on Computer Vision Aided Architectural Design: <a href="https://cvaad-workshop.github.io/" rel="external nofollow noopener" target="_blank">CVAAD</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> International Conference on Learning Representations (ICLR), “Understanding weight-magnitude hyperparameters in training binary networks” [<a href="pub/QuistICLR2023understandWeightMagInBinaryNN.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Boudewine Ossenkoppele. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR workshop L3D-IVU 2023, “What Affects Learned Equivariance in Deep Image Recognition Models?” [<a href="pub/BruintjesCVPRW23LearnedEquivariance.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> Medical Imaging 2023, “Analyzing components of a transformer under different dataset scales in 3D prostate CT segmentation” [<a href="pub/2023_c_SPIEMI.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> Sensors 2023, “Towards single camera human 3D kinematics” [<a href="pub/bittner23sensorsSingleCamKinematics.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at ICCV 2023.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Organization.</b> I’m a reviewer at CVPR 2023.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2023</th> <td> <ul> <li> <b>Accepted paper.</b> WACV 2023, “LAB: Learnable Activation Binarizer for Binary Neural Networks” [<a href="pub/falkenaWACV23lab.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> ICLR 2022, “Flexconv: Continuous kernel convolutions with differentiable kernel sizes” [<a href="pub/flexconv_continuous_kernel_con.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Sadaf Gulshad. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Markus Braun. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Award.</b> British Machine Vision Confence (BMVC): <a href="pub/bmvc22award.jpg">Best student paper</a> (Honorable mention). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2022, “NeRD++: Improved 3D-mirror symmetry</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2022, “Copy-Pasting Coherent Depth Regions Improves Contrastive Learning for</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Mentoring.</b> I am a senior faculty mentor at the <a href="https://eccv2022.ecva.net/program/student-activities/" rel="external nofollow noopener" target="_blank">speed mentoring event</a> at the European Conference on Computer Vision (ECCV). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Dong Hang. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Invited talk.</b> About “AI for image recognition” at <a href="https://www.rvo.nl/" rel="external nofollow noopener" target="_blank">Rijksdienst voor Ondernemend Nederland</a>, in collaboration with <a href="https://allai.nl/" rel="external nofollow noopener" target="_blank">ALLAI</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Organization.</b> Workshop at the European Conference on Computer Vision (ECCV), the 3rd edition of <a href="https://vipriors.github.io/" rel="external nofollow noopener" target="_blank">VIPriors</a>: Visual Inductive Priors for Data-Efficient Deep Learning.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Exited that Dr. Amogh Gudi graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at BMVC 2022.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2022, “Humans disagree with the IoU for measuring object detector localization error” [<a href="pub/strafforelloICIP22humansVersusIoU.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2022, “Heart rate estimation in intense exercise videos” [<a href="pub/napoleonICIP22IntenseHeartRate.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> ICPR 2022, “AmsterTime: A Visual Place Recognition Benchmark Dataset for Severe Domain Shift” [<a href="pub/yildizICPR22AmsterTime.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Happy that Dr. Yancong Lin graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at ECCV 2022.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR 2022, “Deep vanishing point detection: Geometric priors make dataset variations vanish” [<a href="pub/Vanishing_Point_CVPR22.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Supervised PhD graduated.</b> Proud that Dr. Osman Semih Kayhan graduated. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Juan Pedro Vigueras Guillen. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Accepted paper.</b> AAAI 2022, “Equal Bits: Enforcing Equally Distributed Binary Network Weights” [<a href="pub/AAAI2022_Equal_bits_Bnn.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2022</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at CVPR 2022.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Javad Zolfaghari Bengar, CVC, Barcelona. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2021, “Frequency learning for structured CNN filters with Gaussian fractional derivatives” [<a href="pub/Learning_Fractional_Gaussian_Filters___BMVC.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Award.</b> Deep Learning for Geometric Computing 2021 workshop at ICCV: a <a href="pub/andreaBestPaper21.png">best paper</a> award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> (oral) ICCV 2021, “Zero-Shot Day-Night Domain Adaptation With a Physics Prior” [<a href="pub/ICCV2021__Zero_Shot_Domain_Adaptation_with_a_Physics_Prior.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV 2021, “Spectral Leakage and Rethinking the Kernel Size in CNNs” [<a href="pub/Windowing__ICCV_2021.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Organization.</b> Workshop at ICCV, the 2nd edition of <a href="https://vipriors.github.io/" rel="external nofollow noopener" target="_blank">VIPriors</a>: Visual Inductive Priors for Data-Efficient Deep Learning.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> DLGC 2021, “Investigating transformers in the decomposition of polygonal shapes as point collections” [<a href="pub/Alfieri21_polygon_transformers.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> IEEE Biomedical and Health Informatics 2021, “Assessment of Parkinson’s Disease Severity from Videos using Deep Architecture” [<a href="pub/Parkinson21.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2021, “PUNet: Temporal Action Proposal Generation With Positive Unlabeled Learning Using Key Frame Annotations” [<a href="pub/ZiaICIP21punet.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2021, “The Arm-Swing Is Discriminative in Video Gait Recognition for Athlete Re-Identification” [<a href="pub/ChoiICIP21armSwing.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2021, “Exploiting Learned Symmetries in Group Equivariant Convolutions” [<a href="pub/LengyelICIP21learnedSymmetriesGconv.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2021, “Semi-supervised lane detection with Deep Hough Transform” [<a href="pub/linICIP21semiSupervisedLaneDetection.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2021, “Hallucination In Object Detection – A Study In Visual Part Verification” [<a href="pub/KayhanICIP21delftBikes.pdf">pdf</a>, <a href="https://github.com/oskyhn/DelftBikes" rel="external nofollow noopener" target="_blank">DelftBikes</a> data set].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>PhD committee.</b> For the PhD defense of Dr. Shuo Li, TU Delft. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> ICML 2021, “Deep Continuous Networks” [<a href="pub/TomenICML21deepContinuousNetworks.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> TIP 2021, “Resolution learning in deep convolutional networks using scale-space theory” [<a href="pub/Learning_the_Resolution_of_Deep_Convolutional_Neural_Networks.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> AAAI 2021, “Deep Unsupervised Image Hashing by Maximizing Bit Entropy” [<a href="pub/Deep_Unsupervised_Image_Hashing_by_Maximizing_Bit_Entropy.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at CVPR 2021.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at BMVC 2021.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Organization.</b> I’m appointed chairman of the <a href="https://asci.tudelft.nl/?page_id=254" rel="external nofollow noopener" target="_blank">ASCI</a> scientific committee.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR 2021, “No frame left behind: Full Video Action Recognition” [<a href="pub/xinLiuCVPR2021noFrameLeftBehind.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2021</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at ICCV 2021.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Paper reproduction repository.</b> We launched an open website to submit, archive, and structure (partial) reproductions of machine learning papers: <a href="https://reproducedpapers.org/" rel="external nofollow noopener" target="_blank">https://reproducedpapers.org/</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> RRPR 2020, “ReproducedPapers.org: Openly teaching and structuring machine learning reproducibility” [<a href="pub/ReproducedPapers_org.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> MDPI Applied Sciences journal 2020, “Real-time Webcam Heart-Rate and Variability Estimation with Clean Ground Truth for Evaluation” [<a href="pub/gudiMDPI2020heartRate.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> DL-HAU 2020, “t-EVA: Time-Efficient t-SNE Video Annotation” [<a href="pub/t-EVAicprw20.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Invited talk.</b> About “Black Magic in Deep Learning” at <a href="https://www.youtube.com/channel/UCseJlTlqQ2jfW66r-fOYaNg/about" rel="external nofollow noopener" target="_blank">Computer vision talks</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Organization.</b> Workshop at ECCV, the first edition of <a href="https://vipriors.github.io/" rel="external nofollow noopener" target="_blank">VIPriors</a>: Visual Inductive Priors for Data-Efficient Deep Learning.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>PhD committee.</b> Jury member in the committee of Dr. Li, Shuo.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing for CVPR 2021.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing for WACV 2020.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ICPR 2020, “Respecting Domain Relations: Hypothesis Invariance for Domain Generalization” [<a href="pub/domainAlignICPR2020.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ICPR 2020, “WeightAlign: Normalizing Activations by Weight Alignment” [<a href="pub/weightAlignICPR2020.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ICPR 2020, “Zoom-CAM: Generating Fine-grained Pixel Annotations from Image Labels” [<a href="pub/zoomCamICPR2020.pdf">pdf</a>, <a href="https://github.com/X-Shi/Zoom-CAM" rel="external nofollow noopener" target="_blank">code</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ICPR 2020, “Tilting at windmills: Data augmentation for deep pose estimation does not help with occlusions” [<a href="pub/poseOcclusionICPR2020.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing for ICPR 2020.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Youtube.</b> The TU-Delft Computer Vision lab now has its own <a href="https://www.youtube.com/channel/UCTPJRHDiHewQGQUq20qiBnA/" rel="external nofollow noopener" target="_blank">Youtube channel</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Award.</b> OpenEyes ECCV 2020 workshop: Received <a href="https://openeyes-workshop.github.io/#awards" rel="external nofollow noopener" target="_blank">Best paper</a> award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2020, “Black Magic in Deep Learning: How Human Skill Impacts Network Training” [<a href="pub/Black_Magic_in_Deep_LearningBMVC2020.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>TV appearance.</b> On Universiteit van Nederland, <a href="https://www.youtube.com/watch?v=O9n89YcIgCQ" rel="external nofollow noopener" target="_blank">Hoe herkent een zelfrijdende auto een fietser?</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2020, “Deep Hough-Transform Line Priors” [<a href="pub/Hough_Transform_Line_Priors_for_Line_Detection.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> ECCVw 2020, “Efficiency in Real-time Webcam Gaze Tracking” [<a href="pub/Efficiency_in_Real-time_Webcam_Gaze_TrackingECCVw.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> IEEE Access 2020, “On Sensitive Minima in Margin-based Deep Distance Learning” [<a href="pub/sensitive_minima.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Award.</b> CVPR 2020: Received <a href="http://cvpr2020.thecvf.com/" rel="external nofollow noopener" target="_blank">Outstanding reviewer</a> award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Blog post.</b> <a href="https://medium.com/@oskyhn_77789/current-convolutional-neural-networks-are-not-translation-equivariant-2f04bb9062e3" rel="external nofollow noopener" target="_blank">Current Convolutional Neural Networks are not translation equivariant</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR Deep Vision workshop 2020, “Top-Down Networks: A coarse-to-fine reimagination of CNNs” [<a href="pub/LelekasCVPRW20TopDownNetworks.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing for ECCV 2020.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Organization.</b> Organizing an ECCV’20 workshop on <a href="https://vipriors.github.io/" rel="external nofollow noopener" target="_blank">Visual Inductive Priors for Data-Efficient Deep Learning</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR 2020, “On Translation Invariance in CNNs: Convolutional Layers can Exploit Absolute Spatial Location” [<a href="pub/kayhanCVPR20translationInvarianceCNN.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at BMVC 2020.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Speaker.</b> Gave an introduction to CNNs in the context of <a href="http://allai.nl/" rel="external nofollow noopener" target="_blank">AllAI</a> at the Dutch ministry of economic affairs. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Speaker.</b> Guest lecture in the Erasmus <a href="https://faector.nl/students/" rel="external nofollow noopener" target="_blank">Faector</a> excellence program. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2020</th> <td> <ul> <li> <b>Newspaper article.</b> In Dutch national newspaper on difficulty of video search on social media <a href="https://www.volkskrant.nl/nieuws-achtergrond/een-jaar-na-christchurch-zo-n-live-uitgezonden-moordvideo-kan-nog-steeds~b4a27703/" rel="external nofollow noopener" target="_blank">De volkskrant</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing for CVPR 2020.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Slides.</b> on <a href="/HowToDoResearchInDL-slides.pdf">How to do (deep learning) research?</a> Tips, common pitfalls and guidelines.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> I will give a keynote presentation in London, at the British Machine Vision Association’s <a href="https://britishmachinevisionassociation.github.io/meetings/09-25-Video%20Understanding.html" rel="external nofollow noopener" target="_blank">Video understanding</a> workshop.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> I will kick off the first <a href="https://www.meetup.com/Rotterdam-Deep-Learning-Learning-Meetup/" rel="external nofollow noopener" target="_blank">Rotterdam deep learning meetup</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> I will speak at the Delft-Leiden Deep Learning practitioners on how to do (deep learning) research. [<a href="DeepLearningResearch/index.html">slides</a>]<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2019, “Push for Quantization: Deep Fisher Hashing” [<a href="pub/YunqiangLiBMVC19DeepFisherHashing.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV-W 2019, “The Instantaneous Accuracy: a Novel Metric for the Problem of</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV-W 2019, “Cross Domain Image Matching in Presence of Outliers” [<a href="pub/xinLiuICCVWcrossDomainOutliers.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV-W 2019, “Efficient Real-Time Camera Based Estimation of Heart Rate and Its Variability” [<a href="pub/gudiICCVWheartRateVar.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV-W 2019, “Attention-Aware Age-Agnostic Visual Place Recognition” [<a href="pub/wangICCVW19attention4AgeAgnosticPlaceRec.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Program committee.</b> I’m reviewing 8 papers for ICCV 2019.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> ACM MM-W 2019, “Running Event Visualization using Videos from Multiple Cameras” [<a href="pub/napoleonACMMMWrunners.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> I will give a keynote presentation in Bonn, at the <a href="http://ahb2019.cv-uni-bonn.de/#program" rel="external nofollow noopener" target="_blank">Anticipating Human Behavior</a> workshop.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Grant.</b> Obtained a prestigious <a href="https://www.nwo.nl/en/news-and-events/news/2019/05/85-researchers-receive-nwo-vidi-grant-worth-800000-euros.html" rel="external nofollow noopener" target="_blank">NWO VIDI</a> grant on ‘Tabula Inscripta: adding prior knowledge to deep learning’, researching visual inductive priors to improve deep network data efficiency.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Organization.</b> I’m an area chair at <a href="https://bmvc2019.org/" rel="external nofollow noopener" target="_blank">BMVC 2019</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR-W 2019, “Deep Visual City Recognition Visualization” [<a href="pub/xianweiCVPRW19DeepCity.pdf">pdf</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR-W 2019, “ViDeNN: Deep Blind Video Denoising” [<a href="pub/clausCVPRW19videoDenoise.pdf">pdf</a>, <a href="https://github.com/clausmichele/ViDeNN" rel="external nofollow noopener" target="_blank">code</a>].<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Organization.</b> I’ve joined ASCI in the <a href="https://www.asci.tudelft.nl/pages/about-asci/organization.php" rel="external nofollow noopener" target="_blank">research committee</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Organization.</b> I’m a track chair of <a href="https://ict-research.nl/ict-open/programme/tracks/asci-ipa-siks/" rel="external nofollow noopener" target="_blank">ICT.OPEN 2019</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Organization.</b> I’m organizing the “Delft Deep Learning Colloquium 2019” with excellent speakers from industry and academia. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Course.</b> Teaching a 2 day graduate course in June: <a href="http://rstrail.nl/new/event/trail-course-deep-learining-demystified/" rel="external nofollow noopener" target="_blank">Deep Learning Demystified</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> at the “Computer Vision symposium” of Thalia, study association of Nijmegen University. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Speaker.</b> at the <a href="https://www.meetup.com/nl-NL/Delft-AI-Meetup/events/256563603/" rel="external nofollow noopener" target="_blank">Delft AI meetup</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Newspaper article.</b> In Dutch national newspaper discussing Deep learning for sports analysis, <a href="https://www.volkskrant.nl/wetenschap/geen-sport-ontkomt-nog-aan-datadrift~be933517/" rel="external nofollow noopener" target="_blank">De volkskrant: Geen sport ontkomt nog aan datadrift</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>TV appearance.</b> On Dutch national television, “EenVandaag: De vervangbare mens”.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2019</th> <td> <ul> <li> <b>Grant.</b> Obtained a <a href="https://www.nwo.nl/en/news-and-events/news/2019/01/5.1-million-euros-made-available-for-leading-research-in-astronomy-computer-science-and-mathematics.html" rel="external nofollow noopener" target="_blank">NWO TOP</a> grant on ‘pixel-free deep learning’ researching Scale-Space and deep learning.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> TIP 2018, “Divide and Count: Generic Object Counting by Image Divisions” (<a href="pub/stahlTIP18counting.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2018, “Recurrent knowledge distillation” (<a href="pub/pinteaICIP18recurrentDistillation.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Award.</b> ECCV 2018 workshop <a href="https://sites.google.com/view/hands2018/awards" rel="external nofollow noopener" target="_blank">best paper award</a> for “Hand-tremor frequency estimation in videos” (<a href="pub/pinteaECCVW18handTremor.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Workshop.</b> Organizing ECCV’18 workshop <a href="http://ahb2018.cv-uni-bonn.de/" rel="external nofollow noopener" target="_blank">Anticipating Human Behavior</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2018 workshop, “Using phase instead of optical flow for action recognition” (<a href="pub/hommosECCVW18phaseInsteadOfOFforAR.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2018 workshop, “Hand-tremor frequency estimation in videos” (<a href="pub/pinteaECCVW18handTremor.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> PRL 2018, “Asymmetric kernel in Gaussian Processes for learning target variance” (<a href="pub/pinteaPRL18asymmetricGPs.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Accepted paper.</b> International Archives of the Photogrammetry, Remote Sensing &amp; Spatial Information Sciences, “Shape based classification of seismic building structural types”. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Speaker.</b> Keynote speaker at the <a href="https://www.ai-expo.net/europe/speaker/jan-van-gemert/" rel="external nofollow noopener" target="_blank">AI expo Europe</a> (presentation slides: <a href="/assets/pdf/AIexpo-jvgemert.pdf">Deep Learning Bytes</a>). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Seminar.</b> Organizing the “Delft deep learning seminar”. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Seminar.</b> Taught the “Deep Learning Workshop” for TU Delft personell. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Guest lecture</b> at the iQ winterschool 2018 on Machine Learning Applied to Quantitative Analysis of Medical Images. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Interview.</b> Interviewed for <a href="https://www.tudelft.nl/en/eemcs/current/humans-of-eemcs/jan-van-gemert/" rel="external nofollow noopener" target="_blank">Humans of TU Delft</a>. &lt;/br&gt;</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Visiting researcher.</b> I am visiting the <a href="http://www.cvc.uab.es/" rel="external nofollow noopener" target="_blank">Computer Vision Center</a> in Barcelona for six weeks.&lt;/br&gt;</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2018</th> <td> <ul> <li> <b>Grant.</b> Obtained a Take-off grant for visually inspecting aircraft engines.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained funding through a Perpectief grant on “EDL: Efficient Deep Learning”.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained funding through a Perpectief grant on “Citius Altius Sanius: Injury-free exercise for everyone”.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Youtube video of presentation available.</b> For the ICCV 2017 paper “Active Decision Boundary Annotation with Deep Generative Models” (<a href="pub/huijserICCV17ActiveBoundAnnoGAN.pdf">pdf</a>) the <a href="https://www.youtube.com/watch?v=8kKicp7K9Ng" rel="external nofollow noopener" target="_blank">video</a> of the spotlight is online.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Code available.</b> For the ICCV 2017 paper “Active Decision Boundary Annotation with Deep Generative Models” (<a href="pub/huijserICCV17ActiveBoundAnnoGAN.pdf">pdf</a>) we have python code available on <a href="https://github.com/MiriamHu/ActiveBoundary/tree/master" rel="external nofollow noopener" target="_blank">github</a>.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Award.</b> BMVC 2017: Received Outstanding reviewer award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Press release.</b> Nieuwe aanpak bewegingsanalyse blijkt nuttig voor Parkinson, vliegtuigmotoren en sport.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV 2017, “Active Decision Boundary Annotation with Deep Generative Models” (<a href="pub/huijserICCV17ActiveBoundAnnoGAN.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2017, “Object-Extent Pooling for Weakly Supervised Single-Shot Localization” (<a href="pub/gudiBMVC17objectExtentPooling.pdf">pdf</a>).<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained funding for a PhD student from <a href="https://www.nwo.nl/actueel/nieuws/2017/ew/verandering-en-evolutie-staan-centraal-in-big-data-onderzoek.html" rel="external nofollow noopener" target="_blank">NWO Commit2Data</a> for analyzing videos over time. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Award.</b> CVPR 2017: Received <a href="http://cvpr2017.thecvf.com/files/OutstandingReviewers.pdf" rel="external nofollow noopener" target="_blank">Outstanding reviewer</a> award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> VAST 2017, “DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks” (<a href="pub/pezzottiVAST17deepEyes.pdf">pdf</a>). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> ICIAP 2017, “One-Step Time-Dependent Future Video Frame Prediction with a Convolutional Encoder-Decoder Neural Network”. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <table> <tbody> <tr> <td> <b>Accepted paper.</b> CVPR 2017, “Video Acceleration Magnification” (<a href="./pub/videoAccelerationMagnificationCVPR17.pdf">pdf</a> </td> <td> <a href="https://acceleration-magnification.github.io/" rel="external nofollow noopener" target="_blank">Project</a>). <br> </td> </tr> </tbody> </table> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> DLM 2017, “Vision-based Detection of Acoustic Timed Events: a Case Study on Clarinet Note Onsets” (<a href="./pub/visual_Onset_Detection.pdf">pdf</a>, <a href="https://youtu.be/Q7raqQkvWf4" rel="external nofollow noopener" target="_blank">video</a>). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> IJCV 2017, “Tubelets: Unsupervised action proposals from spatiotemporal super-voxels” (<a href="pub/Jain_et_al-2017-International_Journal_of_Computer_Vision.pdf">pdf</a>). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Accepted paper.</b> TIP 2017, “Con-Text: Text Detection for Fine-grained Object Classification”. To appear. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Workshop.</b> Organizing CVPR’17 workshop <a href="http://bravenewmotion.github.io/" rel="external nofollow noopener" target="_blank">Brave new ideas for motion representations in video</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Interview.</b> In “medical delta” about the “Technology In Motion (TIM)” lab.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Postdoc position available.</b> ArchiMediaL: deep learning and knowledge representations. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained funding for a postdoc from “Volkswagen Stiftung” for visually matching various architectural modalities. Project ArchiMediaL together with <a href="http://www.bk.tudelft.nl/en/about-faculty/departments/architecture/organisation/history-of-architecture-urban-planning/" rel="external nofollow noopener" target="_blank">Prof. Carola Hein</a> from Architecture @ TU-Delft, and <a href="http://www.victordeboer.com/" rel="external nofollow noopener" target="_blank">Victor de Boer</a> from Knowledge representations @ VU. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained a pilot grant from <a href="http://sportsengineering.tudelft.nl/" rel="external nofollow noopener" target="_blank">Sports engineering institute</a> for visually analyzing exercising in the gym.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2017</th> <td> <ul> <li> <b>Grant.</b> Obtained a “Take-off” grant for visually analyzing recycling plastic.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Conference.</b> Organizing the Netherlands Conference on Computer Vision (NCCV’16)<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Workshop.</b> Organized ECCV’16 workshop on <a href="http://bravenewmotion.github.io/" rel="external nofollow noopener" target="_blank">Brave new ideas for motion representations in video</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Conference.</b> Co-Organized the European Conference on Computer Vision (<a href="http://www.eccv2016.org/" rel="external nofollow noopener" target="_blank">ECCV’16</a>)<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Data available.</b> Code of the ICCV 2015 paper “Objects2action: Classifying and localizing actions without any video example” (<a href="./pub/jainICCV15Objects2action.pdf">pdf</a>). <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2016 workshop, “Making a Case for Learning Motion Representations with Phase” [<a href="./pub/ECCVw16phase.pdf">pdf</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Code available.</b> of the CVPR 2016 paper “Structured Receptive Fields in CNNs” (<a href="./pub/CVPR2016_StructuredReceptiveFields.pdf">pdf</a>). Caffe and Theano code availabe on <a href="https://github.com/jhjacobsen/RFNN" rel="external nofollow noopener" target="_blank">github</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2016, “Spot On: Action Localization from Pointly-Supervised Proposals” [<a href="./pub/spotOnECCV16.pdf">pdf</a>]. Hollywood2Tubes <a href="./pub/hollywood2tubes.tar.gz">dataset</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> ECCV 2016, “Depth-aware Motion Magnification” [<a href="./pub/depthAwareMotionMagnificationECCV16.pdf">pdf</a>, <a href="./pub/depthAwareMotionMagnificationECCV16supmat.pdf">Supplemental</a>] <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> CVIU 2016, “No Spare Parts: Sharing Part Detectors for Image Categorization” <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Award.</b> Amazon Picking Challenge 2016: The team I advised on deep learning won two tasks. See news articles in <a href="http://www.theverge.com/2016/7/5/12095788/amazon-picking-robot-challenge-2016" rel="external nofollow noopener" target="_blank">The Verge</a> and <a href="https://www.engadget.com/2016/07/05/amazon-robot-picking-challenge-2016-winner/" rel="external nofollow noopener" target="_blank">engadget</a>. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Award.</b> CVPR 2016: Received <a href="http://cvpr2016.thecvf.com/" rel="external nofollow noopener" target="_blank">Outstanding reviewer</a> award. <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2016, “Featureless: Bypassing Feature Extraction in Action Categorization” (<a href="./pub/PinteaICIP16featureless.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> CVIU 2016, “Large Scale Gaussian Process for Overlap-based Object Proposal Scoring” (<a href="./pub/PinteaCVIU16proposalScoring.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR 2016, “Structured Receptive Fields in CNNs” (<a href="./pub/CVPR2016_StructuredReceptiveFields.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Presentation.</b> At the Kivi Kooy symposium of “De plenoptische mens: alle visuele stromen bevaren”.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Poster available.</b> Of the presentation at Delft Data Science and sports, “Computer Vision for Sports” (<a href="./pub/ComputerVisionForSports_DelftDataScience.pdf">Poster</a>)<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2016</th> <td> <ul> <li> <b>Presentation available.</b> Of the oral presentation at BMVC 2015 of “APT: Action localization Proposals from dense Trajectories” (<a href="./pub/gemertBMVC15APTactionProposals.pptx">Presentation</a>)<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Code available.</b> Of “Phase Based Video Motion Processing” as implemented during the Lorentz workshop “ICT with Industry 2015”. Python source code available on <a href="https://github.com/jvgemert/pbMoMa" rel="external nofollow noopener" target="_blank">github</a><br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Data available.</b> Of “What do 15,000 Object Categories Tell Us About Classifying and Localizing Actions?” (<a href="./pub/jain-objects-actions-cvpr2015.pdf">.pdf</a>)<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Code available.</b> Of “APT: Action localization Proposals from dense Trajectories” (<a href="./pub/gemertBMVC15APTactionProposals.pdf">.pdf</a>) python code and pre-computed tubes available on <a href="https://github.com/jvgemert/apt" rel="external nofollow noopener" target="_blank">github</a><br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> ICCV 2015, “Objects2action: Classifying and localizing actions without any video example” (<a href="./pub/jainICCV15Objects2action.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> NCCV 2015, “BING3D: Fast Spatio-Temporal Proposals for Action Localization” (<a href="./pub/gatiNCCV15bing3D.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> MMM 2016, “Exploring the Long Tail of Social Media Tags” (<a href="./pub/kordumovaMM15longtail.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> BMVC 2015, oral presentation, “APT: Action localization Proposals from dense Trajectories” (<a href="./pub/gemertBMVC15APTactionProposals.pdf">.pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>New position:</b> Assistant professor at Delft University of Technology.<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Supervision.</b> Martine van den Berg, graduated on “Mobile Object Recognition: What are the Minimum Criteria that are Acceptable to Museum Visitors?” <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Supervision.</b> Johan Sundin MSc, graduated on “Image Editing from multiple photos to</li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Supervision.</b> June, supervising 8 profile project MSc AI students<br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> CVPR 2015, “What do 15,000 Object Categories Tell Us About Classifying and Localizing Actions?” (<a href="./pub/jain-objects-actions-cvpr2015.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Award.</b> CVPR 2015: Received <a href="http://www.pamitc.org/cvpr15/awards.php" rel="external nofollow noopener" target="_blank"> Outstanding reviewer</a> award <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> ICMR 2015, “Bag-of-Fragments: Selecting and encoding video fragments for event detection and recounting” (<a href="./pub/mettes-bag-of-fragments-icmr2015.pdf">pdf</a>) <br> </li> </ul> </td> </tr> <tr> <th scope="row" style="width: 20%">2015</th> <td> <ul> <li> <b>Accepted paper.</b> ICIP 2015, “Per-patch metric learning for robust image matching” (<a href="./pub/karaogluICIP15steerableInvariance.pdf">pdf</a>) <br> </li> </ul> </td> </tr> </table> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Jan van Gemert. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> </body> </html>